{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alejogiley/Novartis-Hackaton-7/blob/master/Notebooks/Lee_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHn__IVB5T2Q",
        "colab_type": "text"
      },
      "source": [
        "# Predicting affinities of antibiotic candidates to a DNA Gyrase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDipX-nMAOFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget -c https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh\n",
        "#!chmod +x Anaconda3-5.1.0-Linux-x86_64.sh\n",
        "#!bash ./Anaconda3-5.1.0-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "#!conda install -q -y --prefix /usr/local -c omnia --no-update-deps pdbfixer=1.4\n",
        "#!conda install -q -y --prefix /usr/local -c conda-forge --no-update-deps xgboost=0.6a2\n",
        "#!conda install -q -y --prefix /usr/local -c rdkit --no-update-deps rdkit=2017.09.1\n",
        "#!conda install -q -y --prefix /usr/local -c deepchem --no-update-deps  deepchem-gpu=2.1.0\n",
        "\n",
        "#import sys\n",
        "#sys.path.append('/usr/local/lib/python3.6/site-packages/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ksekxnts6Lq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93ef1944-e4e5-4e5a-991e-8ea99af70539"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy  as np      # scientific computing: arrays\n",
        "import scipy  as sp      # scientific computing: statistics\n",
        "import pandas as pd      # data analysis tools\n",
        "\n",
        "# Tensor operations\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "# Neural Network\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, BatchNormalization\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Data processing\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpLI3gGSyIBL",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network\n",
        "\n",
        "A simple Fully Connected Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P55bkQmyfdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU info\n",
        "# !nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7_WVRSM7XJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = \"https://raw.githubusercontent.com/alejogiley/Novartis-Hackaton-7/master/Data/Gyrase/AZ_Pyrrolamides_features_final.csv\"\n",
        "datasets = pd.read_csv(filepath)\n",
        "\n",
        "# rename\n",
        "datasets['pIC50'] = datasets['SAU Gyr IC50 (ç¤›)']\n",
        "\n",
        "pattern = \"[<]\"\n",
        "filters = datasets.pIC50.str.contains(pattern)\n",
        "datasets[\"left_saturated\"] = filters\n",
        "\n",
        "pattern = \"[>]\"\n",
        "filters = datasets.pIC50.str.contains(pattern)\n",
        "datasets[\"right_saturated\"] = filters\n",
        "\n",
        "# Reorder dataframe\n",
        "cols = datasets.columns.tolist()\n",
        "cols = cols[:2] + cols[-2:] + cols[3:-2]\n",
        "datasets = datasets[cols]\n",
        "\n",
        "datasets['pIC50'] = datasets['pIC50'].str.replace(r'[><]', '')\n",
        "datasets['pIC50'] = datasets['pIC50'].apply(lambda x: np.log(float(x)))\n",
        "\n",
        "# drop Ipc\n",
        "datasets = datasets.drop(['Ipc'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trlVz01uluno",
        "colab_type": "text"
      },
      "source": [
        "Select __X__ and __Y__ vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No5ySwjxjjRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input and output\n",
        "y = datasets['pIC50'].copy()\n",
        "y = y.astype('float64').to_numpy()\n",
        "\n",
        "x = datasets.iloc[:,5:].copy()\n",
        "x = x.astype('float64').to_numpy()\n",
        "\n",
        "# qualifiers classification\n",
        "s1 = datasets['left_saturated' ].apply(lambda x: x*1).copy()\n",
        "s2 = datasets['right_saturated'].apply(lambda x: x*1).copy()\n",
        "\n",
        "s1 = s1.to_numpy()\n",
        "s2 = s2.to_numpy()\n",
        "\n",
        "s0 = s1 + s2\n",
        "\n",
        "# > greater\n",
        "rcutoff = s2 * datasets['pIC50'].copy().astype('float64')\n",
        "rcutoff = rcutoff.to_numpy()\n",
        "\n",
        "# < lower\n",
        "lcutoff = s1 * datasets['pIC50'].copy().astype('float64')\n",
        "lcutoff = lcutoff.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_uM-OUUn9ld",
        "colab_type": "text"
      },
      "source": [
        "Split the machine-learning-ready dataset into __training__, __test__ and __validation__ subsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3X88qpukiBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG7zUqyZt6rc",
        "colab_type": "text"
      },
      "source": [
        "The performance of common machine-learning algorithms can be very sensitive to preprocessing of the data, neural networks mostly. Here we will normalize the features and $\\log{\\text{IC50}}$ to have zero-mean and unit-standard-deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODU3luRZ2bvE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "dc2b7dcc-dcdf-48f9-b900-ec2245b9dc06"
      },
      "source": [
        "# input dimensions\n",
        "number_of_features = x.shape[1]\n",
        "\n",
        "# Start neural network\n",
        "network = Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(Dense(units=50, input_shape=(number_of_features, )))\n",
        "network.add(BatchNormalization())\n",
        "network.add(Activation(\"relu\"))\n",
        "network.add(Dropout(0.2))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(Dense(units=100))\n",
        "network.add(Activation(\"relu\"))\n",
        "network.add(Dropout(0.2))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(Dense(units=100))\n",
        "network.add(Activation(\"relu\"))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(Dense(units=25))\n",
        "network.add(Activation(\"relu\"))\n",
        "\n",
        "# Add fully connected layer with a sigmoid activation function\n",
        "network.add(Dense(units=1, activation='linear'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0820 20:08:02.070174 139709010372480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0820 20:08:02.089701 139709010372480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0820 20:08:02.099619 139709010372480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0820 20:08:02.214973 139709010372480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0820 20:08:02.249144 139709010372480 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adH-v8aYok6h",
        "colab_type": "text"
      },
      "source": [
        "The customized Loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kDEaySUojyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    # get deltas\n",
        "    z = y_pred - y_true\n",
        "    r = y_pred - rcutoff\n",
        "    l = y_pred - lcutoff\n",
        "    # qualifiers adjusted Loss function\n",
        "    return K.mean((1-s0)*K.square(z), axis=-1)\n",
        "    #return K.mean((1-s0)*K.square(z) + s2*K.relu(-r) + s1*K.relu(l), axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6AqbU5hpdr1",
        "colab_type": "text"
      },
      "source": [
        "### Run Lola, Run\n",
        "\n",
        "Parameters are not optimized!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Ce2Fu0pu_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "47d715e0-0151-4434-bdff-2ea02736677f"
      },
      "source": [
        "# Compile neural network\n",
        "network.compile(loss=custom_loss,                     # Custom loss function\n",
        "                optimizer=Adam(lr=1e-2, decay=1e-4),  # Adam optimizer\n",
        "                metrics=['mse'])                      # Accuracy performance metric"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0820 20:08:02.408785 139709010372480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbIZ4mlFF6eQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "21803a22-f423-44c5-b705-04d6d071a522"
      },
      "source": [
        "# Train neural network\n",
        "history = network.fit(x_train,                          # Features\n",
        "                      y_train,                          # Target\n",
        "                      epochs=150,                       # Number of epochs\n",
        "                      verbose=0,                        # No output\n",
        "                      batch_size=10,                    # Number of observations per batch\n",
        "                      validation_data=(x_test, y_test)) # Data for evaluation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0820 20:08:02.832729 139709010372480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j08gPVT-Uf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (10,5)    # plot size\n",
        "\n",
        "# Get training and test loss histories\n",
        "training_loss = history.history['loss']\n",
        "test_loss = history.history['val_loss']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "# Visualize loss history\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, test_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Test Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZkapqVuI9gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (20,10)    # plot size\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "c = y_test.astype(int)\n",
        "ax.scatter(y_test, network.predict(x_test), \n",
        "           s=65, c=c, cmap=plt.cm.coolwarm, \n",
        "           zorder=10, edgecolors='k')\n",
        "\n",
        "ax.set_xlabel(\"pIC50 experimental\", fontsize=18)\n",
        "ax.set_ylabel(\"pIC50 prediction\",   fontsize=18)\n",
        "\n",
        "lims = [\n",
        "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
        "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
        "]\n",
        "\n",
        "# now plot both limits against eachother\n",
        "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
        "ax.set_aspect('equal')\n",
        "ax.set_xlim(lims)\n",
        "ax.set_ylim(lims)\n",
        "\n",
        "# integer limits\n",
        "ilims = [int(x+0.5) for x in lims]\n",
        "ax.set_xticks(np.arange(*ilims,2))\n",
        "ax.set_yticks(np.arange(*ilims,2))\n",
        "\n",
        "# We change the fontsize of minor ticks label \n",
        "ax.tick_params(axis='both', which='major', labelsize=18)\n",
        "ax.tick_params(axis='both', which='major', labelsize=18)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5QIAbuopRgY",
        "colab_type": "text"
      },
      "source": [
        "### Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzg9XfkCSgVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import linregress, kendalltau\n",
        "\n",
        "tau, p_value = kendalltau(y_test, network.predict(x_test)[:,0])\n",
        "slope, intercept, r_value, p_value, std_err = linregress(y_test, network.predict(x_test)[:,0])\n",
        "\n",
        "print(\"R2 value: %s\" % r_value)\n",
        "print(\"Kendall Tau value: %s\" % tau)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}